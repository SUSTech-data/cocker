dependncies:
  - pip:
      - https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.6/flash_attn-2.5.6+cu118torch2.2cxx11abiFALSE-cp310-cp310-linux_x86_64.whl
      - -v -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers
